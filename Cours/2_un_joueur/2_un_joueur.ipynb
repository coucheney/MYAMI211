{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jeux séquentiels à 1 joueur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Horizon fini, sans hasard\n",
    "\n",
    "---\n",
    "\n",
    "Représentation sous forme d'arbre:\n",
    "* **sommets internes** (ou sommet décisionnel): états du jeu où le joueur prend une décision\n",
    "* **sommets feuilles**: états de fin de jeu\n",
    "* **arcs**: transitions entre états, avec récompense $r_{x,y}$ pour l'arc $(x,y)$\n",
    "\n",
    "\n",
    "\n",
    "<center><img src='fig/tree_simple.png'  style=\"width: 1000px;\"></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stratégie\n",
    "\n",
    "---\n",
    "\n",
    "**Stratégie**: choix (déterministe) d'une transition pour chaque sommet décisionnel. La valeur de la stratégie est la valeur de la feuille reliée à la racine par cette stratégie.\n",
    "\n",
    "**Questions**:\n",
    "* dans l'exemple, quelle est la valeur de la stratégie qui choisit toujours la transition vers le haut?\n",
    "* dans l'exemple, combien y a-t'il de stratégies différentes?\n",
    "* et combien de stratégies dans le jeu de Sokoban à $n$ positions, un joueur et 5 caisses à déplacer?\n",
    "\n",
    "\n",
    "<center><img src='fig/tree_simple.png'  style=\"width: 1000px;\"></center> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stratégie optimale\n",
    "\n",
    "---\n",
    "\n",
    "Une stratégie est **optimale** si sa valeur est maximale parmi l'ensemble des stratégies.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La **valeur d'un sommet** $x$, notée $v(x)$, est la valeur d'une stratégie optimale dans le **sous-jeu** défini par le sous-arbre issu de $x$. Alors:\n",
    "\n",
    "$$v(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "0 \\text{ si }x\\text{ est une feuille}\\\\\n",
    "\\max_{(x,y) \\in A}(r_{x,y} + v(y)) \\text{ sinon}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "**Questions**: \n",
    "\n",
    "* preuve?\n",
    "\n",
    "* que peut-on dire des stratégies aléatoires?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algorithme backward induction de calcul des valeurs\n",
    "\n",
    "---\n",
    "\n",
    "**Principe**: on calcule récursivement les valeurs depuis les feuilles, ainsi que les stratégies optimales associées.\n",
    "\n",
    "<center><img src='fig/tree_simple_resolu.png'  style=\"width: 1000px;\"></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Horizon fini, avec hasard\n",
    "\n",
    "---\n",
    "\n",
    "**Exemple**: on lance un dé, si on fait 1 ou 2, on ne gagne rien (on perd tous les points cumulés) et le jeu s'arrête, si on fait 3 ou 4 on gagne 1 point, si on fait 5 ou 6, on gagne 2 points. Si on fait 5 ou 6, on peut relancer le dé, et cela au plus deux fois. Calculer alors l'espérance de gain maximale que l'on peut obtenir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Trois types de sommets:\n",
    "* sommets feuilles\n",
    "* sommets décisionnels\n",
    "* sommets hasard\n",
    "\n",
    "Calcul récursif de la valeur d'un sommet:\n",
    "\n",
    "$$v(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "0 \\text{ si }x\\text{ est une feuille}\\\\\n",
    "\\max_{(x,y) \\in A}(r_{x,y} + v(y)) \\text{ si c'est un sommet décisionnel}\\\\\n",
    "\\sum_{(x,y) \\in A} P_{xy}(r_{x,y} + v(y)) \\text{ si c'est un sommet hasard}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice\n",
    "\n",
    "---\n",
    "\n",
    "On considère un jeu dans lequel une urne contient des boules blanches et des boules noires. A chaque étape, et tant qu'il reste des boules dans l'urne, le joueur choisit soit de poursuivre soit d’arrêter le jeu. S’il décide de continuer alors on tire une boule au hasard uniformément. Si c’est une boule noire le joueur perd 1 point, et si c’est une boule blanche le joueur gagne 1 point. Quand le joueur s'arrête, éventuellement parce qu'il n'y a plus de boules dans l'urne, il gagne le nombre de points qu'il a cumulés au cours de la partie. Le joueur connaît le nombre de boules blanches et de boules noires au début du jeu.\n",
    "\n",
    "* Proposer une modélisation du jeu (définir les états, les transitions et leurs récompenses).\n",
    "* Donner les relations permettant de calculer la valeur des états.\n",
    "* Combien le joueur peut-il espérer gagner au mieux s'il y a, au début, 2 boules blanches et 3 boules noires? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Autre exercice: le problème de la location de logement\n",
    "\n",
    "---\n",
    "\n",
    "Connu sous le nom *problème du secrétaire*:\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Probl%C3%A8me_du_secr%C3%A9taire\n",
    "\n",
    "\n",
    "* on visite au plus $n$ logements à louer dans un ordre aléatoire\n",
    "* la visite permet de savoir si le logement est mieux que ceux visités avant\n",
    "* juste après la visite, on accepte ou non la location, et la décision est irrévocable\n",
    "* si on a rejeté les $n-1$ premiers logements, alors on accepte le $n$-ème\n",
    "* **objectif**: maximiser la probabilité de choisir le meilleur logement\n",
    "* Définir les états, calculer la valeur de ces états, et donner une stratégie optimale.\n",
    "* Indication: commencer par traiter les cas $n=1,2,3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Modélisation**:\n",
    "* états: $(i, b)$ avec $1\\leq i \\leq n$ et $b \\in \\{0,1\\}$\n",
    "    * $b$ vaut 1 si le $i$-ème logement est meilleur que tous les précédents, et 0 sinon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='fig/secretary_pb.png'></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='fig/secretary_pb.png'></center> \n",
    "\n",
    "**Calcul de la valeur**:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "v(i,0) & = \\frac{i}{i+1} v(i+1, 0) + \\frac{1}{i+1} v(i+1, 1) \\\\\n",
    "v(i,1) & = \\max[\\frac in,  v(i,0)] \\\\\n",
    "v(n,1) & = 1 \\\\\n",
    "v(n,0) & = 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37427501364792026, 0.37427501364792026, 0.37427501364792026, 0.37427501364792026, 0.37427501364792026, 0.3742750136479202, 0.37427501364792026, 0.37427501364792026, 0.37427501364792026, 0.37427501364792026, 0.37427501364792026, 0.3742750136479203, 0.3742750136479203, 0.3742750136479203, 0.3742750136479203, 0.3742750136479203, 0.3742750136479203, 0.3742750136479203, 0.38, 0.4, 0.42, 0.44, 0.46, 0.48, 0.5, 0.52, 0.54, 0.56, 0.58, 0.6, 0.62, 0.64, 0.66, 0.68, 0.7, 0.72, 0.74, 0.76, 0.78, 0.8, 0.82, 0.84, 0.86, 0.88, 0.9, 0.92, 0.94, 0.96, 0.98, 1]\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "v0, v1 = [0]*n, [0]*n\n",
    "v1[-1] = 1\n",
    "for i in range(n-2, -1, -1):\n",
    "    v0[i] = (i+1)/(i+2)*v0[i+1] + 1/(i+2)*v1[i+1]\n",
    "    v1[i] = max((i+1)/n, v0[i])\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice \n",
    "\n",
    "---\n",
    "\n",
    "On considère un jeu où un joueur tire à pile ou face avec une pièce qui a une probabilité $p$ de tomber sur face. Si le joueur obtient pile, il ajoute 1 point à son score. Le joueur décide de continuer ou de s'arrêter avant chaque lancer de pièce. Si le joueur s'arrête, il gagne autant d'argent que de points accumulés. Si la pièce tombe sur face le jeu s'arrête et le joueur perd alors tous les points accumulés, c'est-à-dire qu'il ne gagne rien. \n",
    "\n",
    "* Modéliser ce jeu\n",
    "* Ecrire les équations d'optimalité\n",
    "* Montrer que la stratégie à seuil suivante est optimale: \"si le joueur a plus de $\\frac{1-p}{p}$ points, alors il s'arrête, sinon il continue\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Horizon infini avec hasard: Processus de décision Markovien (MDP pour faire court)\n",
    "\n",
    "---\n",
    "\n",
    "C'est un processus de récompense Markovien avec un agent qui va contrôler certaines transitions afin de maximiser le gain total:\n",
    "\n",
    "* ensemble fini d'états $\\mathcal{S} = \\mathcal{S}_r \\cup \\mathcal{S}_d$\n",
    "    * $\\mathcal{S}_r$ est l'ensemble des **états aléatoires** (peut contenir des puits)\n",
    "    * $\\mathcal{S}_d$ est l'ensemble des **états décisionnels**\n",
    "* probabilités de transition pour tout état aléatoire $s \\in \\mathcal{S}_r$ et tout $s' \\in \\mathcal{S}_r \\cup \\mathcal{S}_d$:\n",
    "$$P_{ss'} = \\mathbb{P}[S_{t+1} = s' | S_t = s] $$\n",
    "* **pour tout état décisionnel $s \\in \\mathcal{S}_d$, on note $\\mathcal{A}(s) \\subseteq \\mathcal{S}$ l'ensemble des actions d'un agent qui sont les transitions possibles depuis l'état $s$**\n",
    "* fonction de récompense $r_{ss'}$ pour toute transition $s \\rightarrow s'$\n",
    "* facteur de discount $\\gamma \\in [0,1]$\n",
    "* **objectif: stratégie qui maximise le gain discounté en espérance (la valeur)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemple de la machine à café \n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='fig/machine_cafe.png'  style=\"width: 1000px;\"></center> \n",
    "\n",
    "\n",
    "* Etats décisionnels: *Neuve* et *Abimée*\n",
    "* Etats aléatoires: *N,ent*, *N,pas* et *A,pas*\n",
    "* Puits: *Cassée*\n",
    "* discount $\\gamma = 1$ (graphe stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stratégie Markovienne déterministe $\\pi$\n",
    "\n",
    "---\n",
    "\n",
    "Choix d'une transition pour chaque état décisionnel\n",
    "\n",
    "$$ \\forall s \\in \\mathcal{S}_d, \\pi(s) \\in A(s) $$\n",
    "\n",
    "\n",
    "* dépend uniquement de l'état (stratégie Markovienne)\n",
    "* ne varie pas au cours du temps (stratégie stationnaire)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La stratégie $\\pi$ étant fixée, la suite des état $(S_t)_{t \\geq 0}$ induit un processus de récompense Markovien, où les probabilités de transition sont:\n",
    "\n",
    "$$P^\\pi_{ss'} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "P_{ss'} & \\text{ si }s \\in \\mathcal{S}_r\\\\\n",
    "1 & \\text{ si }s \\in \\mathcal{S}_d \\text{ et } s' = \\pi(s) \\\\\n",
    "0 & \\text{ sinon}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Valeur d'une stratégie\n",
    "\n",
    "---\n",
    "\n",
    "Pour tout état décisionnel $s$, la valeur $v_\\pi$ de la stratégie $\\pi$ est caractérisée par la relation linéaire suivante:\n",
    "\n",
    "\n",
    "\n",
    "$$v_\\pi(s) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\sum_{s' \\in \\mathcal{S}}P_{ss'}(r_{ss'} + \\gamma v_\\pi(s')) & \\text{ si }s \\in \\mathcal{S}_r\\\\\n",
    "r_{s,\\pi(s)} + \\gamma v_\\pi(\\pi(s)) & \\text{ si }s \\in \\mathcal{S}_d \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Qui s'écrit matriciellement:\n",
    "\n",
    "$$v_\\pi = R^\\pi + \\gamma P^\\pi v_\\pi$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Valeurs des stratégies Markoviennes déterministes de la machine à café\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='fig/machine_cafe.png'  style=\"width: 1000px;\"></center> \n",
    "\n",
    "Calculer la valeur de la stratégie déterministe où l'on entretient toujours la machine à café.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|  | E,E  | P,E  | E,P  | P,P \n",
    "|:--- |:---:|:---:|:---:|:---:\n",
    "|**Neuve**  |  40 | 50  | 22.5 | 23.3|\n",
    "|*N,ent*  |  38 | 47.5  | 20.5 | 21.2|\n",
    "|*N,pas*  |  36 | 45  | 17.9 | 18.3|\n",
    "|**Abimée**  |  40 | 50  | 16.7 | 16.7|\n",
    "|*A,pas*  |  28 | 35  | 11.6 | 11.6|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Valeur optimale\n",
    "\n",
    "---\n",
    "\n",
    "La valeur optimale $v^*$ est définie par\n",
    "$$v^*(s) = \\max_\\pi v_\\pi(s), \\forall s \\in \\mathcal{S}_d $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "Propriétés\n",
    "</div>\n",
    "\n",
    "* il existe une stratégie **déterministe optimale** $\\pi^*$ telle que: $v_{\\pi^*} = v^*  $\n",
    "* en particulier elle est optimale pour tout état initial $s \\in \\mathcal{S}_d$\n",
    "* (admis) cette stratégie est meilleure que n'importe quelle stratégie non Markovienne et/ou aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "Résoudre un MDP:\n",
    "</div>\n",
    "\n",
    "* calculer la valeur optimale\n",
    "* ou une stratégie optimale\n",
    "* passer de l'un à l'autre est \"facile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Equation d'optimalité de Bellman \n",
    "---\n",
    "\n",
    "Pour tout état $s \\in \\mathcal{S}_d$, la valeur optimale est caractérisée par les équations de Bellman\n",
    "\n",
    "$$v^*(s) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\sum_{s' \\in \\mathcal{S}}P_{ss'}(r_{ss'} + \\gamma v^*(s')) & \\text{ si }s \\in \\mathcal{S}_r\\\\\n",
    "\\displaystyle \\max_{s' \\in A(s)} (r_{ss'} + \\gamma v^*(s')) & \\text{ si }s \\in \\mathcal{S}_d \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "* généralise la caractérisation de la valeur dans le cas acyclique.\n",
    "* équations non linéaires à cause de la fonction max\n",
    "\n",
    "**Exercice**: écrire les équations d'optimalité dans l'exemple de la machine à café.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calcul d'une politique optimale à partir de la valeur optimale $v^*$\n",
    "\n",
    "---\n",
    "\n",
    "Une stratégie optimale est telle que pour chaque état décisionnel $s \\in \\mathcal{S}_d$ on choisit une action qui maximise\n",
    "\n",
    "$$\n",
    "\\max_{s' \\in A(s)} (r_{ss'} + \\gamma v^*(s'))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En effet, la valeur de cette stratégie est alors $v^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithmes de résolution des MDP\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert-info\">\n",
    "    \n",
    "avec connaissance du modèle (programmation dynamique)\n",
    "</div>\n",
    "\n",
    "* forme close de la solution (cas rare)\n",
    "* itération de valeur\n",
    "* programmation linéaire\n",
    "* itération de stratégie\n",
    "    \n",
    "    \n",
    "<div class=\"alert-info\">\n",
    "    \n",
    "    \n",
    "sans connaissance du modèle\n",
    "</div>\n",
    "\n",
    "* itération de stratégie avec estimation par Monte Carlo\n",
    "* extension de TD(0) au cadre décisionnel: SARSA, q-learning\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Itération de valeur\n",
    "\n",
    "---\n",
    "\n",
    "Equation d'optimalité de Bellman:\n",
    "\n",
    "$$v^*(s) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\sum_{s' \\in \\mathcal{S}}P_{ss'}(r_{ss'} + \\gamma v^*(s')) & \\text{ si }s \\in \\mathcal{S}_r\\\\\n",
    "\\displaystyle \\max_{s' \\in A(s)} (r_{ss'} + \\gamma v^*(s')) & \\text{ si }s \\in \\mathcal{S}_d \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Soit $f: \\mathbb{R}^{\\mathcal{S}} \\rightarrow \\mathbb{R}^{\\mathcal{S}} $ définie par\n",
    "\n",
    "$$f(w)(s) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\sum_{s' \\in \\mathcal{S}}P_{ss'}(r_{ss'} + \\gamma w(s')) & \\text{ si }s \\in \\mathcal{S}_r\\\\\n",
    "\\displaystyle \\max_{s' \\in A(s)} (r_{ss'} + \\gamma w(s')) & \\text{ si }s \\in \\mathcal{S}_d \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Qui s'écrit sous forme matricielle:\n",
    "\n",
    "$$ f(w) = \\max_{\\pi \\text{ déterministe}}   R^\\pi + \\gamma P^\\pi w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Propriétés admises**:\n",
    "\n",
    "* $f$ est une fonction contractante si $\\gamma < 1$:\n",
    "$$\\|f(w_1) - f(w_2) \\|_\\infty \\leq \\gamma \\|w_1 - w_2 \\|_\\infty $$\n",
    "* la valeur optimale $v^*$ est l'unique **point fixe** de $f$;\n",
    "* alors toute suite $(w_k)_k$ définie par $w_{k+1} = f(w_k)$ converge vers $v^*$ (algorithme d'itération de valeur);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Résolution par programmation linéaire\n",
    "\n",
    "---\n",
    "Equation d'optimalité de Bellman:\n",
    "$$v^*(s) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\sum_{s' \\in \\mathcal{S}}P_{ss'}(r_{ss'} + \\gamma v^*(s')) & \\text{ si }s \\in \\mathcal{S}_r\\\\\n",
    "\\displaystyle \\max_{s' \\in A(s)} (r_{ss'} + \\gamma v^*(s')) & \\text{ si }s \\in \\mathcal{S}_d \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Remarque**: le calcul de la fonction $\\max(x,y)$ peut se faire par programmation linéaire:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\min z\\\\\n",
    "z \\geq x\\\\\n",
    "z \\geq y\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La valeur optimale est alors la valeur du programme linéaire suivant:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\min \\sum_{s \\in \\mathcal{S}} w(s)&\\\\\n",
    "w(s) = \\sum_{s' \\in \\mathcal{S}}P_{ss'}(r_{ss'} + \\gamma w(s')) & \\forall s \\in \\mathcal{S}_r\\\\\n",
    "\\displaystyle w(s) \\geq r_{ss'} + \\gamma w(s') & \\forall s \\in \\mathcal{S}_d , \\forall s' \\in A(s)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "**Exercice**: Ecrire le programme linéaire pour l'exemple de la machine à café."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La preuve repose sur la propriété suivante: si $w \\geq f(w)$ alors $w \\geq v^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propriété des valeurs\n",
    "\n",
    "---\n",
    "\n",
    "Si $w \\geq f(w)$ alors $w \\geq v^*$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "    \n",
    "   preuve\n",
    "</div>\n",
    "\n",
    "Supposons $w \\geq f(w)$, il suffit de montrer $w \\geq v_\\pi$ pour toute stratégie déterministe $\\pi$. Soit $\\bar{\\pi}$ une telle stratégie.\n",
    "\n",
    "On a \n",
    "\n",
    "$$v_\\bar\\pi = R^\\bar\\pi + \\gamma P^\\bar\\pi v_\\bar\\pi$$\n",
    "\n",
    "Alors\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "w & \\geq \\displaystyle \\max_{\\pi \\text{ déterministe}}   R^\\pi + \\gamma P^\\pi w \\\\\n",
    "& \\geq R^\\bar\\pi + \\gamma P^\\bar\\pi w\\\\\n",
    "& \\geq R^\\bar\\pi + \\gamma P^\\bar\\pi (R^\\bar\\pi + \\gamma P^\\bar\\pi w)\\\\\n",
    "& \\geq R^\\bar\\pi + \\gamma P^\\bar\\pi R^\\bar\\pi + \\gamma^2 (P^\\bar\\pi)^2 R^\\bar\\pi + \\dots + \\gamma^n (P^\\bar\\pi)^n w\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "La dernière quantité tend vers $v_\\bar\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Itération de stratégie\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Soit $\\pi$ une stratégie déterministe.\n",
    "* on calcule $v_\\pi$\n",
    "* si $v_\\pi = f(v_\\pi)$ alors $v_\\pi$ est la valeur optimale et $\\pi$ est une stratégie optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sinon calculer $\\pi \\leftarrow \\text{ greedy}(v_\\pi)$ et on recommence\n",
    "    * une stratégie **greedy par rapport à $v_\\pi$**  est une stratégie qui  maximise\n",
    "\n",
    "$$ \\max_{\\bar \\pi \\text{ déterministe}}   R^{\\bar \\pi} + \\gamma P^{\\bar \\pi} v_\\pi $$\n",
    "\n",
    "**Exercice**: tester sur l'exemple de la machine à café.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preuve de convergence de l'itération de stratégie\n",
    "\n",
    "    \n",
    "---\n",
    "\n",
    "Soit $\\pi'$ la stratégie obtenue après itération de la stratégie $\\pi$ non optimale:\n",
    "\n",
    "* alors $v_{\\pi'} > v_\\pi$ \n",
    "* le nombre de stratégies étant fini, cet algo termine sur une stratégie $\\pi^*$ telle que $v_{\\pi^*}$ satisfait l'équation d'optimalité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "Preuve de $v_{\\pi'} > v_\\pi$ \n",
    "</div>\n",
    "\n",
    "On considère la suite de stratégies non stationnaires $(\\pi_i)_{i \\geq 0}$ où $\\pi_i$ est la stratégie $\\pi'$ jusqu'au temps $t=i-1$ puis la stratégie $\\pi$.\n",
    "\n",
    "* $\\pi_0 = \\pi$\n",
    "* $\\pi_\\infty = \\pi'$\n",
    "\n",
    "et soit $v_i$ les valeurs associées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On a:\n",
    "\n",
    "$$v_{i+1} =  R^{\\pi'} + \\gamma P^{\\pi'} v_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preuve de convergence de l'itération de stratégie (suite)\n",
    "\n",
    "    \n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Par construction, $v_1 = f(v_\\pi)$, et:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "f(v_\\pi) & \\displaystyle = \\max_{\\bar\\pi \\text{ déterministe}}   R^\\bar\\pi + \\gamma P^\\bar\\pi v_\\pi \\\\\n",
    "& \\geq R^{\\pi} + \\gamma P^{\\pi} v_\\pi\\\\\n",
    "& = v_\\pi \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "L'inégalité est stricte car sinon $\\pi$ serait optimal donc\n",
    "\n",
    "$$v_1 > v_0 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On montre **par récurrence** $v_i > v_0$ pour tout $i \\geq 1$:\n",
    "\n",
    "* vrai pour $i=1$\n",
    "* si vrai jusqu'à $i$, alors\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "v_{i+1} & \\displaystyle = R^{\\pi'} + \\gamma P^{\\pi'} v_i \\\\\n",
    "& > R^{\\pi'} + \\gamma P^{\\pi'} v_0\\\\\n",
    "& = v_1 \\\\\n",
    "& > v_0\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
